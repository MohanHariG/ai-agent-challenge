{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# AutoScholar Research Assistant (Kaggle)\n**Capstone:** Multi-agent research assistant — search (SerpAPI) → extract → cite → summarize (Gemini)\nAuthor: MOHAN HARI G\nDate: 2025-11-20","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q requests beautifulsoup4 lxml tldextract rouge-score python-dateutil tenacity","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport time\nimport hashlib\nimport logging\nfrom pathlib import Path\nfrom typing import List, Dict, Any\nimport requests\nfrom bs4 import BeautifulSoup\nimport tldextract\nimport re\nfrom datetime import datetime\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# Optional rouge scorer import (used in evaluation cell)\ntry:\n    from rouge_score import rouge_scorer\nexcept Exception:\n    rouge_scorer = None\n\n# Basic configuration\nDATA_DIR = Path(\"data\")\nCACHE_DIR = DATA_DIR / \"cached_results\"\nDATA_DIR.mkdir(exist_ok=True)\nCACHE_DIR.mkdir(exist_ok=True)\nMEM_FILE = DATA_DIR / \"memory.json\"\nLOG_FILE = DATA_DIR / \"logs.json\"\n\n# Logging setup\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\nlogger = logging.getLogger(\"autoscholar\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nprint(\"GEMINI key present in os.environ:\", bool(os.environ.get(\"GEMINI_API_KEY\")))\n\n# Try to import google.generativeai and configure (do not print the key)\ntry:\n    import google.generativeai as genai\n    print(\"google.generativeai imported OK. genai module:\", genai)\n    try:\n        genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n        print(\"Configured genai with GEMINI_API_KEY (did not print key).\")\n    except Exception as e:\n        print(\"genai.configure() raised:\", repr(e))\nexcept Exception as e:\n    print(\"Import google.generativeai failed:\", repr(e))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    from kaggle_secrets import UserSecretsClient\n    import os\n    usc = UserSecretsClient()\n    serp = None\n    gem = None\n    try:\n        serp = usc.get_secret(\"SERPAPI_API_KEY\")\n    except Exception as e:\n        # secret missing or not created\n        serp = None\n    try:\n        gem = usc.get_secret(\"GEMINI_API_KEY\")\n    except Exception as e:\n        gem = None\n\n    if serp:\n        os.environ[\"SERPAPI_API_KEY\"] = serp\n    if gem:\n        os.environ[\"GEMINI_API_KEY\"] = gem\n\n    print(\"SERPAPI_API_KEY loaded into environment:\", bool(os.environ.get(\"SERPAPI_API_KEY\")))\n    print(\"GEMINI_API_KEY loaded into environment:\", bool(os.environ.get(\"GEMINI_API_KEY\")))\nexcept Exception as e:\n    print(\"kaggle_secrets not available or permission denied:\", e)\n    print(\"If you're running outside Kaggle, add the keys to os.environ or Kaggle Secrets.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SERPAPI_API_KEY = os.environ.get(\"SERPAPI_API_KEY\")\nGEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n\nif not SERPAPI_API_KEY:\n    logger.warning(\"SERPAPI_API_KEY not found in environment. Add it to Kaggle Secrets before running search.\")\nif not GEMINI_API_KEY:\n    logger.warning(\"GEMINI_API_KEY not found in environment. Summarizer calls will fail until it's set.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _hash_text(s: str) -> str:\n    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()[:16]\n\ndef save_json(path: Path, obj: Any):\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(json.dumps(obj, indent=2, ensure_ascii=False))\n\ndef load_json(path: Path, default=None):\n    if path.exists():\n        return json.loads(path.read_text())\n    return default\n\ndef safe_get_text(soup: BeautifulSoup) -> str:\n    if soup is None:\n        return \"\"\n    # Remove unwanted tags\n    for tag in soup([\"script\", \"style\", \"noscript\", \"iframe\"]):\n        tag.decompose()\n    text = soup.get_text(separator=\"\\n\")\n    # collapse whitespace\n    text = re.sub(r\"\\n\\s*\\n+\", \"\\n\\n\", text)\n    return text.strip()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SERPAPI_SEARCH_URL = \"https://serpapi.com/search\"\n\ndef serpapi_search(query: str, num_results: int = 5, use_cache: bool = True) -> List[Dict[str, Any]]:\n    \"\"\"\n    Query SerpAPI (Google engine) and return organic results as dicts.\n    Caches results to data/cached_results/<hash>.json\n    \"\"\"\n    qhash = _hash_text(f\"serp::{query}::{num_results}\")\n    cache_path = CACHE_DIR / f\"{qhash}.json\"\n    if use_cache and cache_path.exists():\n        logger.info(f\"Loading cached SerpAPI results for query: {query}\")\n        return load_json(cache_path, [])\n\n    params = {\n        \"engine\": \"google\",\n        \"q\": query,\n        \"num\": num_results,\n        \"api_key\": SERPAPI_API_KEY\n    }\n    try:\n        resp = requests.get(SERPAPI_SEARCH_URL, params=params, timeout=15)\n        resp.raise_for_status()\n        data = resp.json()\n    except Exception as e:\n        logger.error(f\"SerpAPI request failed: {e}\")\n        return []\n\n    results = []\n    # SerpAPI returns organic_results (may differ for location/engine)\n    for item in data.get(\"organic_results\", [])[:num_results]:\n        results.append({\n            \"title\": item.get(\"title\"),\n            \"url\": item.get(\"link\") or item.get(\"url\"),\n            \"snippet\": item.get(\"snippet\") or item.get(\"snippet_highlighted_words\") or \"\"\n        })\n    # fallback: serpapi sometimes returns 'top' or 'answer_box' - ignore for now\n\n    save_json(cache_path, results)\n    logger.info(f\"SerpAPI: {len(results)} results saved to cache for query: {query}\")\n    return results\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def fetch_page(url: str, timeout: int = 10) -> Dict[str, Any]:\n    \"\"\"Fetch a URL, return dict with title and text (fallbacks handled).\"\"\"\n    try:\n        headers = {\n            \"User-Agent\": \"Mozilla/5.0 (compatible; AutoScholar/1.0; +https://example.com/bot)\"\n        }\n        r = requests.get(url, headers=headers, timeout=timeout)\n        r.raise_for_status()\n        html = r.text\n        soup = BeautifulSoup(html, \"lxml\")\n        title = (soup.title.string if soup.title else \"\") or \"\"\n        text = safe_get_text(soup)\n        return {\"title\": title.strip(), \"text\": text.strip(), \"url\": url}\n    except Exception as e:\n        logger.warning(f\"Failed to fetch {url}: {e}\")\n        return {\"title\": \"\", \"text\": \"\", \"url\": url}\n\ndef extract_top_passages(page_text: str, min_chars: int = 300) -> List[str]:\n    \"\"\"\n    Simple heuristic: split by paragraphs and return the longest passages\n    useful for summarization, up to some limit.\n    \"\"\"\n    if not page_text:\n        return []\n    paras = [p.strip() for p in page_text.split(\"\\n\") if len(p.strip()) > 50]\n    # sort paras by length\n    paras_sorted = sorted(paras, key=len, reverse=True)\n    # return top 3 paragraphs that meet min_chars\n    top = [p for p in paras_sorted if len(p) >= min_chars][:3]\n    return top\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def domain_score(url: str) -> float:\n    \"\"\"\n    Very simple domain trust heuristic:\n    - educational domains (.edu) get +0.3\n    - known news domains get +0.2 (simple heuristic)\n    - otherwise 0\n    \"\"\"\n    try:\n        ext = tldextract.extract(url)\n        suffix = ext.suffix or \"\"\n        domain = ext.domain or \"\"\n        score = 0.0\n        if suffix.endswith(\"edu\") or domain.endswith(\"edu\"):\n            score += 0.3\n        # quick-news heuristic (you can expand)\n        if domain.lower() in {\"nytimes\", \"theguardian\", \"bbc\"}:\n            score += 0.2\n        return score\n    except Exception:\n        return 0.0\n\ndef overlap_score(snippet: str, query: str) -> float:\n    # simple token overlap ratio\n    s_tokens = set(re.findall(r\"\\w+\", snippet.lower()))\n    q_tokens = set(re.findall(r\"\\w+\", query.lower()))\n    if not q_tokens:\n        return 0.0\n    overlap = s_tokens.intersection(q_tokens)\n    return len(overlap) / max(1, len(q_tokens))\n\ndef rank_sources(results: List[Dict[str, Any]], query: str) -> List[Dict[str, Any]]:\n    \"\"\"\n    Score sources by (overlap_score * weight) + domain_score and sort.\n    Returns list with added 'score' key.\n    \"\"\"\n    scored = []\n    for r in results:\n        snippet = r.get(\"snippet\") or \"\"\n        url = r.get(\"url\") or \"\"\n        base_score = overlap_score(snippet, query)\n        dscore = domain_score(url)\n        total = base_score * 0.7 + dscore * 0.3\n        scored.append({**r, \"score\": total})\n    scored_sorted = sorted(scored, key=lambda x: x[\"score\"], reverse=True)\n    return scored_sorted\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    import google.generativeai as genai\n    genai.configure(api_key=GEMINI_API_KEY)\n    _HAS_GEMINI = True\nexcept Exception:\n    _HAS_GEMINI = False\n    logger.warning(\"google.generativeai not available. Install library or ensure GEMINI_API_KEY is set.\")\n\nSUMMARY_PROMPT_TEMPLATE = \"\"\"\nYou are a research assistant. Given extracted passages and their metadata, produce:\n1) A factual, unbiased ~500-word summary synthesizing the main points.\n2) A concise 5-bullet outline (each bullet 1–2 sentences).\n3) A short list of the 3 citations (title + url + confidence).\n\nPassages:\n{passages}\n\nCitations metadata:\n{citations}\n\nReturn JSON with keys: summary, outline (list), citations (list of {title,url,note}).\n\"\"\"\n\ndef call_gemini_summarizer(passages, citations, max_tokens=2048):\n    \"\"\"\n    Calls Gemini and returns dict with {summary, outline, citations}.\n    \"\"\"\n\n    if not _HAS_GEMINI:\n        raise RuntimeError(\"Gemini client not available. Install google.generativeai and set GEMINI_API_KEY.\")\n\n    # Build the citation lines safely\n    citation_lines = []\n    for c in citations:\n        title = c.get(\"title\", \"\")\n        url = c.get(\"url\", \"\")\n        score = c.get(\"score\", 0)\n        citation_lines.append(f\"{title}: {url} (score={score:.2f})\")\n\n    prompt = SUMMARY_PROMPT_TEMPLATE.format(\n        passages=\"\\n\\n---\\n\\n\".join(passages),\n        citations=\"\\n\".join(citation_lines)\n    )\n\n    # Gemini call\n    response = genai.generate(\n        model=\"gemini-1.0\",\n        input=prompt,\n        max_output_tokens=max_tokens\n    )\n\n    # Extract text safely\n    try:\n        if hasattr(response, \"text\"):\n            text = response.text\n        else:\n            text = str(response)\n    except:\n        text = str(response)\n\n    # Try to extract JSON\n    try:\n        import re, json\n        m = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n        if m:\n            return json.loads(m.group(0))\n    except:\n        pass\n\n    # Fallback if JSON not detected\n    summary = text.strip()\n    sentences = re.split(r'(?<=[.!?])\\s+', summary)\n    outline = sentences[:5]\n\n    return {\n        \"summary\": summary,\n        \"outline\": outline,\n        \"citations\": [\n            {\"title\": c.get(\"title\",\"\"), \"url\": c.get(\"url\",\"\"), \"note\": f\"score={c.get('score',0):.2f}\"}\n            for c in citations[:3]\n        ]\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Orchestrator:\n    def __init__(self, search_fn, fetch_fn, rank_fn, summarize_fn, max_search_results=6, workers=4):\n        self.search_fn = search_fn\n        self.fetch_fn = fetch_fn\n        self.rank_fn = rank_fn\n        self.summarize_fn = summarize_fn\n        self.max_search_results = max_search_results\n        self.workers = workers\n        # memory & logs\n        self.memory = load_json(MEM_FILE, {\"sessions\": []})\n        self.logs = load_json(LOG_FILE, [])\n\n    def _log(self, event: Dict[str, Any]):\n        event[\"ts\"] = datetime.utcnow().isoformat()\n        self.logs.append(event)\n        # keep log file writing cheap\n        save_json(LOG_FILE, self.logs)\n\n    def run(self, topic: str) -> Dict[str, Any]:\n        run_id = _hash_text(f\"run::{topic}::{time.time()}\")\n        self._log({\"event\": \"start_run\", \"topic\": topic, \"run_id\": run_id})\n        # 1) search (parallel)\n        self._log({\"event\": \"search_start\", \"topic\": topic})\n        results = self.search_fn(topic, num_results=self.max_search_results)\n        self._log({\"event\": \"search_end\", \"result_count\": len(results)})\n\n        # 2) fetch + extract (parallel)\n        self._log({\"event\": \"fetch_start\", \"result_count\": len(results)})\n        pages = []\n        with ThreadPoolExecutor(max_workers=self.workers) as ex:\n            futures = {ex.submit(self.fetch_fn, r.get(\"url\")): r for r in results if r.get(\"url\")}\n            for fut in as_completed(futures):\n                src = futures[fut]\n                try:\n                    page = fut.result()\n                    # attach the original snippet/title if missing\n                    if not page.get(\"title\"):\n                        page[\"title\"] = src.get(\"title\",\"\")\n                    if not page.get(\"text\"):\n                        page[\"text\"] = src.get(\"snippet\",\"\")\n                    pages.append({**src, **page})\n                except Exception as e:\n                    logger.warning(f\"Error fetching {src.get('url')}: {e}\")\n        self._log({\"event\": \"fetch_end\", \"fetched\": len(pages)})\n\n        # 3) extract top passages to summarize\n        passages = []\n        for p in pages:\n            top = extract_top_passages(p.get(\"text\",\"\"))\n            for t in top:\n                passages.append({\"text\": t, \"source\": {\"title\": p.get(\"title\"), \"url\": p.get(\"url\"), \"snippet\": p.get(\"snippet\", \"\")}})\n        # fallback: if no passages, use snippets\n        if not passages:\n            for r in results:\n                passages.append({\"text\": r.get(\"snippet\",\"\"), \"source\": {\"title\": r.get(\"title\"), \"url\": r.get(\"url\")}})\n\n        # 4) citation selection/ranking\n        ranked = self.rank_fn(results, topic)\n        top_citations = ranked[:3]\n\n        # 5) summarizer\n        # Prepare passages as plain text segments for Gemini\n        passages_texts = [p[\"text\"] for p in passages][:10]  # limit to avoid extremely long inputs\n        self._log({\"event\": \"summarizer_start\", \"passage_count\": len(passages_texts)})\n        try:\n            summary_out = self.summarize_fn(passages_texts, top_citations)\n        except Exception as e:\n            logger.error(f\"Summarizer error: {e}\")\n            summary_out = {\"summary\": \"ERROR: summarizer failed.\", \"outline\": [], \"citations\": [{\"title\": c.get(\"title\",\"\"), \"url\": c.get(\"url\",\"\")} for c in top_citations]}\n\n        self._log({\"event\": \"summarizer_end\", \"summary_len\": len(summary_out.get(\"summary\",\"\")) if summary_out.get(\"summary\") else 0})\n\n        # 6) persist to memory\n        session_record = {\n            \"run_id\": run_id,\n            \"topic\": topic,\n            \"timestamp\": datetime.utcnow().isoformat(),\n            \"summary\": summary_out.get(\"summary\",\"\"),\n            \"outline\": summary_out.get(\"outline\", []),\n            \"citations\": summary_out.get(\"citations\", [])\n        }\n        self.memory.setdefault(\"sessions\", []).append(session_record)\n        save_json(MEM_FILE, self.memory)\n        self._log({\"event\": \"run_end\", \"run_id\": run_id})\n\n        # final structured output\n        return {\n            \"topic\": topic,\n            \"summary\": summary_out.get(\"summary\",\"\"),\n            \"outline\": summary_out.get(\"outline\", []),\n            \"citations\": summary_out.get(\"citations\", []),\n            \"meta\": {\"results_found\": len(results), \"fetched_pages\": len(pages)}\n        }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"orchestrator = Orchestrator(\n    search_fn=serpapi_search,\n    fetch_fn=fetch_page,\n    rank_fn=rank_sources,\n    summarize_fn=call_gemini_summarizer,\n    max_search_results=6,\n    workers=4\n)\nlogger.info(\"Orchestrator created. Ready to run.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport google.generativeai as genai\nimport json, re, traceback\n\ngenai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n\ndef call_gemini_summarizer(passages, citations, max_tokens=2048):\n    # Normalize citations\n    safe_citations = []\n    for c in citations or []:\n        if isinstance(c, dict):\n            safe_citations.append({\n                \"title\": c.get(\"title\",\"\"),\n                \"url\": c.get(\"url\",\"\"),\n                \"score\": float(c.get(\"score\",0) or 0)\n            })\n    if not safe_citations:\n        safe_citations = [{\"title\":\"No source found\",\"url\":\"\",\"score\":0.0}]\n    \n    citation_lines = [\n        f\"{c['title']}: {c['url']} (score={c['score']:.2f})\"\n        for c in safe_citations\n    ]\n    \n    prompt = SUMMARY_PROMPT_TEMPLATE.format(\n        passages=\"\\n\\n---\\n\\n\".join(passages or [\"\"]),\n        citations=\"\\n\".join(citation_lines)\n    )\n    \n    try:\n        # Use correct Gemini API for your environment\n        model = genai.GenerativeModel(\"gemini-pro\")\n        response = model.generate_content(prompt)\n\n        # Text output\n        text = response.text\n\n        # Try extract JSON\n        m = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n        if m:\n            parsed = json.loads(m.group(0))\n            parsed.setdefault(\"summary\",\"\")\n            parsed.setdefault(\"outline\", [])\n            parsed.setdefault(\"citations\", [])\n\n            return parsed\n\n        # Fallback: simple text summary\n        summary = text.strip()\n        sentences = re.split(r'(?<=[.!?])\\s+', summary)\n        outline = sentences[:5]\n\n        return {\n            \"summary\": summary,\n            \"outline\": outline,\n            \"citations\": [\n                {\"title\": c[\"title\"], \"url\": c[\"url\"], \"note\": f\"score={c['score']:.2f}\"}\n                for c in safe_citations[:3]\n            ]\n        }\n\n    except Exception as e:\n        print(\"Gemini error:\", e)\n        # Extractive fallback\n        joined = \"\\n\\n\".join(passages or [])\n        summary = joined[:1200] or \"No content to summarize.\"\n        sents = re.split(r'(?<=[.!?])\\s+', summary)\n        outline = sents[:5]\n        return {\n            \"summary\": summary,\n            \"outline\": outline,\n            \"citations\": [\n                {\"title\": c[\"title\"], \"url\": c[\"url\"], \"note\": f\"score={c['score']:.2f}\"}\n                for c in safe_citations[:3]\n            ]\n        }\n\nprint(\"Replaced summarizer with GenerativeModel-based version.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"out = orchestrator.run(\"Impact of Generative AI on developer productivity\")\nprint(out[\"summary\"][:1000])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom pprint import pprint\nlogs = load_json(LOG_FILE, []) or []\nprint(\"Total log events:\", len(logs))\ngem_errs = [e for e in logs if e.get(\"event\") in (\"gemini_error\",\"gemini_exception\",\"gemini_call_failed\")]\nprint(\"gemini_error-like events:\", len(gem_errs))\nfor e in gem_errs[-10:]:\n    print(\"=\"*80)\n    print(\"ts:\", e.get(\"ts\"))\n    print(\"event:\", e.get(\"event\"))\n    print(\"error:\", e.get(\"error\"))\n    trace = e.get(\"trace\") or e.get(\"traceback\") or e.get(\"tb\") or \"\"\n    if trace:\n        print(\"trace (trimmed):\")\n        print(trace[:2000])\n    else:\n        # sometimes error stored as plain message\n        print(\"raw:\", e)\nprint(\"=\"*80)\nprint(\"\\nLast 20 generic log events (most recent last):\")\nfor ev in logs[-20:]:\n    print(ev.get(\"ts\",\"\")[:19], ev.get(\"event\"), ev.get(\"topic\",\"\"), ev.get(\"summary_len\", ev.get(\"result_count\",\"\")))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport traceback, inspect\ntry:\n    import google.generativeai as genai\n    genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n    print(\"Using google.generativeai version:\", getattr(genai, \"__version__\", \"unknown\"))\n    # Try a short prompt and capture full response object\n    model_name_candidates = [\"gemini-pro\", \"gemini-2.5-flash\" ,\"gemini-1.0\", \"gemini-1.5\", \"gemini-1.5-pro\", \"gemini-1.0-lite\", \"chat-bison\"]\n    tried = []\n    resp = None\n    # try the model you used earlier first\n    for model_name in [ \"gemini-2.5-flash\", \"gemini-1.0\" ] + model_name_candidates:\n        try:\n            print(\"\\nTrying model:\", model_name)\n            model = genai.GenerativeModel(model_name)\n            # Use a very short prompt to reduce cost and time\n            resp = model.generate_content(\"Say hello in one sentence.\", max_output_tokens=32)\n            print(\"Call succeeded with model:\", model_name)\n            break\n        except Exception as e:\n            print(\" -> call failed for\", model_name, \":\", repr(e))\n            tried.append((model_name, repr(e)))\n            continue\n\n    if resp is None:\n        print(\"All model attempts failed. Summary of attempts:\")\n        for t in tried:\n            print(t[0], \":\", t[1])\n    else:\n        print(\"\\n=== RAW RESPONSE REPR ===\")\n        print(repr(resp)[:2000])\n        print(\"\\n=== TYPE ===\")\n        print(type(resp))\n        print(\"\\n=== dir(resp) sample ===\")\n        attr_names = [a for a in dir(resp) if not a.startswith(\"_\")]\n        print(attr_names[:60])\n        # print any obvious text attributes\n        for key in (\"text\",\"content\",\"result\",\"candidates\",\"candidates_text\",\"content_text\"):\n            if hasattr(resp, key):\n                try:\n                    print(f\"\\nATTRIBUTE resp.{key} (preview):\")\n                    val = getattr(resp, key)\n                    print(repr(val)[:2000])\n                except Exception as e:\n                    print(\"error reading attribute\", key, e)\n        # try to iterate any top-level fields\n        try:\n            # some response objects behave like mappings\n            items = list(resp.__dict__.items())[:20]\n            print(\"\\nresp.__dict__ sample (trimmed):\")\n            for k,v in items:\n                print(k, \":\", repr(v)[:400])\n        except Exception:\n            pass\nexcept Exception:\n    print(\"Exception in diagnostic:\")\n    traceback.print_exc()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef extractive_only_summarizer(passages, citations, max_tokens=1024):\n    # create a ~500-word extractive summary from passages\n    joined = \"\\n\\n\".join(passages or [])\n    if not joined.strip():\n        return {\"summary\":\"No passages available to summarize.\",\"outline\":[],\"citations\":[{\"title\":c.get(\"title\",\"\"),\"url\":c.get(\"url\",\"\"),\"note\":c.get(\"note\",\"\")} for c in (citations or [])[:3]]}\n    paras = [p.strip() for p in joined.split(\"\\n\\n\") if p.strip()]\n    paras_sorted = sorted(paras, key=len, reverse=True)\n    word_target = 500\n    summary_parts = []\n    current_words = 0\n    for p in paras_sorted:\n        wc = len(p.split())\n        if current_words >= word_target:\n            break\n        summary_parts.append(p)\n        current_words += wc\n    # if still not enough, fill with remaining text truncated\n    if current_words < word_target:\n        # just take joined truncated\n        summary_text = (joined[:4000]).strip()\n    else:\n        summary_text = \"\\n\\n\".join(summary_parts)\n    # outline: pick 5 longest sentences\n    sents = re.split(r'(?<=[.!?])\\s+', summary_text)\n    sents_sorted = sorted([s for s in sents if s.strip()], key=len, reverse=True)\n    outline = [s.strip() for s in sents_sorted[:5]]\n    # normalize citations\n    norm_cits = []\n    for c in (citations or [])[:3]:\n        if isinstance(c, dict):\n            norm_cits.append({\"title\": c.get(\"title\",\"\"), \"url\": c.get(\"url\",\"\"), \"note\": c.get(\"note\", f\"score={c.get('score','')}\")})\n        else:\n            norm_cits.append({\"title\": str(c), \"url\": \"\", \"note\": \"\"})\n    return {\"summary\": summary_text, \"outline\": outline, \"citations\": norm_cits}\n\n\ncall_gemini_summarizer = extractive_only_summarizer\norchestrator.summarize_fn = call_gemini_summarizer  if hasattr(orchestrator, \"summarize_fn\") else None\nprint(\"Now using extractive-only summarizer. Re-run a demo topic to verify output.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndemo_topics = [\n    \"Impact of Generative AI on developer productivity\",\n    \"Edge computing vs cloud computing for smart city applications\",\n    \"Use of LLMs in healthcare diagnostics — opportunities and risks\",\n    \"Climate change: projected sea-level rise in South and Southeast Asia\",\n    \"Cybersecurity best practices for small businesses\"\n]\n\n\nfor topic in demo_topics:\n    print(\"\\n\" + \"=\"*80)\n    print(\"TOPIC:\", topic)\n    out = orchestrator.run(topic)\n    print(\"\\n--- SUMMARY (first 800 chars) ---\\n\")\n    print(out[\"summary\"][:800] + (\"\\n...\" if len(out[\"summary\"])>800 else \"\\n\"))\n    print(\"\\n--- OUTLINE ---\")\n    for b in out[\"outline\"][:5]:\n        print(\"-\", b)\n    print(\"\\n--- CITATIONS ---\")\n    for c in out[\"citations\"][:3]:\n        if isinstance(c, dict):\n            print(f\"- {c.get('title','')} — {c.get('url','')} (note: {c.get('note', '') or c.get('score', '')})\")\n        else:\n            print(\"-\", c)\n    print(\"\\nMeta:\", out[\"meta\"])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport pandas as pd\n\ndef export_human_eval_csv(topics_outputs: List[Dict[str, Any]], out_path: str = \"data/human_eval_template.csv\"):\n    rows = []\n    for o in topics_outputs:\n        rows.append({\n            \"topic\": o[\"topic\"],\n            \"summary\": o[\"summary\"],\n            \"outline\": \" | \".join(o[\"outline\"]),\n            \"citation1\": o[\"citations\"][0][\"url\"] if o[\"citations\"] and len(o[\"citations\"])>0 else \"\",\n            \"citation2\": o[\"citations\"][1][\"url\"] if o[\"citations\"] and len(o[\"citations\"])>1 else \"\",\n            \"citation3\": o[\"citations\"][2][\"url\"] if o[\"citations\"] and len(o[\"citations\"])>2 else \"\",\n            \"relevance\": \"\", \"coherence\": \"\", \"citations_correctness\": \"\", \"notes\": \"\"\n        })\n    df = pd.DataFrame(rows)\n    df.to_csv(out_path, index=False)\n    return df\n\n# Save human-eval CSV for all demo topics (reads from orchestrator.memory)\noutputs = orchestrator.memory.get(\"sessions\", [])[-len(demo_topics):] if orchestrator.memory.get(\"sessions\") else []\ndf = export_human_eval_csv(outputs, out_path=\"data/human_eval_template.csv\")\nprint(\"Human-eval CSV saved to data/human_eval_template.csv\")\ndf.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nif rouge_scorer is None:\n    print(\"rouge_score library not available. Install `rouge-score` to compute automatic ROUGE metrics.\")\nelse:\n    # Example: If you have a dict `references` mapping topic->reference_summary\n    references = {}  # fill in if you have references\n    scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'], use_stemmer=True)\n    rows = []\n    for s in orchestrator.memory.get(\"sessions\", []):\n        topic = s[\"topic\"]\n        pred = s[\"summary\"]\n        ref = references.get(topic)\n        if not ref:\n            continue\n        scores = scorer.score(ref, pred)\n        rows.append({\"topic\": topic, **{k: v.fmeasure for k,v in scores.items()}})\n    if rows:\n        import pandas as pd\n        df = pd.DataFrame(rows)\n        print(df.describe().T)\n    else:\n        print(\"No reference summaries provided; skipping ROUGE.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Recent sessions saved:\", len(orchestrator.memory.get(\"sessions\", [])))\n# show last 2 sessions\nfor s in orchestrator.memory.get(\"sessions\", [])[-2:]:\n    print(\"->\", s[\"topic\"], \"|\", s[\"timestamp\"], \"| summary_len:\", len(s.get(\"summary\",\"\")))\n    \n# Show logs (limited)\nlogs = load_json(LOG_FILE, [])\nprint(f\"Loaded {len(logs)} log events. Last 10:\")\nfor ev in logs[-10:]:\n    print(ev.get(\"ts\"), ev.get(\"event\"), ev.get(\"topic\", \"\"), ev.get(\"summary_len\", ev.get(\"result_count\", \"\")))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_json(MEM_FILE, orchestrator.memory)\nsave_json(LOG_FILE, orchestrator.logs)\nprint(\"Memory and logs saved to data/\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pprint import pprint\nimport pandas as pd\n\n# 1) define the single topic (first of the demo list)\ntopic = \"Impact of Generative AI on developer productivity\"\n\nprint(f\"Running AutoScholar on single topic:\\n-> {topic}\\n\")\nout = orchestrator.run(topic)\n\n# 2) Pretty-print results (summary preview + outline + citations)\nprint(\"\\n\" + \"=\"*80)\nprint(\"SUMMARY (first 1200 chars):\\n\")\nprint(out[\"summary\"][:1200] + (\"\\n...\" if len(out[\"summary\"])>1200 else \"\\n\"))\nprint(\"\\n\" + \"-\"*40)\nprint(\"OUTLINE:\")\nfor i, b in enumerate(out.get(\"outline\", [])[:5], 1):\n    print(f\"{i}. {b}\")\n\nprint(\"\\n\" + \"-\"*40)\nprint(\"CITATIONS:\")\nfor i, c in enumerate(out.get(\"citations\", [])[:3], 1):\n    title = c.get(\"title\",\"\")\n    url = c.get(\"url\",\"\")\n    note = c.get(\"note\",\"\") or c.get(\"score\",\"\")\n    print(f\"{i}. {title}\\n   {url}\\n   note: {note}\")\n\nprint(\"\\nMeta:\", out.get(\"meta\"))\nprint(\"=\"*80 + \"\\n\")\n\n# 3) Export human-eval CSV (single row)\nrow = {\n    \"topic\": out[\"topic\"],\n    \"summary\": out[\"summary\"],\n    \"outline\": \" | \".join(out.get(\"outline\", [])),\n    \"citation1\": out.get(\"citations\", [])[0].get(\"url\",\"\") if out.get(\"citations\") else \"\",\n    \"citation2\": out.get(\"citations\", [])[1].get(\"url\",\"\") if len(out.get(\"citations\", []))>1 else \"\",\n    \"citation3\": out.get(\"citations\", [])[2].get(\"url\",\"\") if len(out.get(\"citations\", []))>2 else \"\",\n    \"relevance\": \"\", \"coherence\": \"\", \"citations_correctness\": \"\", \"notes\": \"\"\n}\ndf = pd.DataFrame([row])\ndf.to_csv(\"data/human_eval_single_topic.csv\", index=False)\nprint(\"Human-eval CSV saved to: data/human_eval_single_topic.csv\")\ndisplay(df.head(1))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport pandas as pd\n\nlogs = load_json(LOG_FILE, []) or []\ndf_logs = pd.DataFrame(logs[-50:])\nprint(\"Showing last 50 log events:\")\ndf_logs\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\n\nif not logs:\n    print(\"No logs to display.\")\nelse:\n    times = [l.get(\"ts\") for l in logs]\n    events = [l.get(\"event\") for l in logs]\n\n    plt.figure(figsize=(14,4))\n    plt.plot(range(len(times)), range(len(times)), alpha=0)  # dummy axis\n\n    for i, (t, e) in enumerate(zip(times, events)):\n        plt.scatter(i, 0, s=40)\n        plt.text(i, 0.03, e, rotation=45, fontsize=8, ha='right')\n\n    plt.title(\"Timeline of Agent Events\")\n    plt.yticks([])\n    plt.xlabel(\"Event Index (chronological)\")\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom collections import Counter\n\nevent_counts = Counter([l.get(\"event\") for l in logs])\nprint(\"Tool / Event usage counts:\\n\")\nfor k, v in event_counts.items():\n    print(f\"{k:20s} : {v}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport pandas as pd\nfrom datetime import datetime\n\ndef parse_ts(ts):\n    try:\n        return datetime.fromisoformat(ts)\n    except:\n        return None\n\ndf = pd.DataFrame(logs)\ndf[\"dt\"] = df[\"ts\"].apply(parse_ts)\n\nlatencies = {}\n\n# stage: search\nstart = df[df.event == \"search_start\"][\"dt\"].max()\nend   = df[df.event == \"search_end\"][\"dt\"].max()\nif start and end:\n    latencies[\"search_ms\"] = (end - start).total_seconds() * 1000\n\n# stage: fetch\nstart = df[df.event == \"fetch_start\"][\"dt\"].max()\nend   = df[df.event == \"fetch_end\"][\"dt\"].max()\nif start and end:\n    latencies[\"fetch_ms\"] = (end - start).total_seconds() * 1000\n\n# stage: summarizer\nstart = df[df.event == \"summarizer_start\"][\"dt\"].max()\nend   = df[df.event == \"run_end\"][\"dt\"].max()\nif start and end:\n    latencies[\"summarizer_ms\"] = (end - start).total_seconds() * 1000\n\nlatencies","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.core.display import display, HTML\nfrom jupyter_server.serverapp import list_running_servers\n\n\n# Gets the proxied URL in the Kaggle Notebooks environment\ndef get_adk_proxy_url():\n    PROXY_HOST = \"https://kkb-production.jupyter-proxy.kaggle.net\"\n    ADK_PORT = \"8000\"\n\n    servers = list(list_running_servers())\n    if not servers:\n        raise Exception(\"No running Jupyter servers found.\")\n\n    baseURL = servers[0][\"base_url\"]\n\n    try:\n        path_parts = baseURL.split(\"/\")\n        kernel = path_parts[2]\n        token = path_parts[3]\n    except IndexError:\n        raise Exception(f\"Could not parse kernel/token from base URL: {baseURL}\")\n\n    url_prefix = f\"/k/{kernel}/{token}/proxy/proxy/{ADK_PORT}\"\n    url = f\"{PROXY_HOST}{url_prefix}\"\n\n    styled_html = f\"\"\"\n    <div style=\"padding: 15px; border: 2px solid #f0ad4e; border-radius: 8px; background-color: #fef9f0; margin: 20px 0;\">\n        <div style=\"font-family: sans-serif; margin-bottom: 12px; color: #333; font-size: 1.1em;\">\n            <strong>⚠️ IMPORTANT: Action Required</strong>\n        </div>\n        <div style=\"font-family: sans-serif; margin-bottom: 15px; color: #333; line-height: 1.5;\">\n            The ADK web UI is <strong>not running yet</strong>. You must start it in the next cell.\n            <ol style=\"margin-top: 10px; padding-left: 20px;\">\n                <li style=\"margin-bottom: 5px;\"><strong>Run the next cell</strong> (the one with <code>!adk web ...</code>) to start the ADK web UI.</li>\n                <li style=\"margin-bottom: 5px;\">Wait for that cell to show it is \"Running\" (it will not \"complete\").</li>\n                <li>Once it's running, <strong>return to this button</strong> and click it to open the UI.</li>\n            </ol>\n            <em style=\"font-size: 0.9em; color: #555;\">(If you click the button before running the next cell, you will get a 500 error.)</em>\n        </div>\n        <a href='{url}' target='_blank' style=\"\n            display: inline-block; background-color: #1a73e8; color: white; padding: 10px 20px;\n            text-decoration: none; border-radius: 25px; font-family: sans-serif; font-weight: 500;\n            box-shadow: 0 2px 5px rgba(0,0,0,0.2); transition: all 0.2s ease;\">\n            Open ADK Web UI (after running cell below) ↗\n        </a>\n    </div>\n    \"\"\"\n\n    display(HTML(styled_html))\n\n    return url_prefix\n\n\nprint(\"✅ Helper functions defined.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!adk create AutoScholar_Research_Assistant_agent --model gemini-2.5-flash-lite --api_key $GEMINI_API_KEY","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google.adk.agents import LlmAgent\nfrom google.adk.models.google_llm import Gemini\n\nfrom google.genai import types\n\n# Configure Model Retry on errors\nretry_config = types.HttpRetryOptions(\n    attempts=5,  # Maximum retry attempts\n    exp_base=7,  # Delay multiplier\n    initial_delay=1,\n    http_status_codes=[429, 500, 503, 504],  # Retry on these HTTP errors\n)\n\ndef set_device_status(location: str, device_id: str, status: str) -> dict:\n    \"\"\"Sets the status of a smart home device.\n\n    Args:\n        location: The room where the device is located.\n        device_id: The unique identifier for the device.\n        status: The desired status, either 'ON' or 'OFF'.\n\n    Returns:\n        A dictionary confirming the action.\n    \"\"\"\n    print(f\"Tool Call: Setting {device_id} in {location} to {status}\")\n    return {\n        \"success\": True,\n        \"message\": f\"Successfully set the {device_id} in {location} to {status.lower()}.\"\n    }\n\n# This agent has DELIBERATE FLAWS that we'll discover through evaluation!\nroot_agent = LlmAgent(\n    model=Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config),\n    name=\"AutoScholar_Research_Assistant_agent\",\n    description=\"A multi-agent research system that searches, extracts, summarizes, and cites reliable information—automating end-to-end topic analysis.\",\n    instruction=\"\"\"AutoScholar works by taking any topic you give it and starting a full research workflow automatically. It searches the web, collects useful information, extracts key points, and generates a clear summary with citations. You only need to run the notebook cell with your topic, and the agent handles everything else on its own. When it finishes, you can read the summary, check the outline, see the sources, and view the logs of how the agent worked.\"\"\",\n    tools=[set_device_status],\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"url_prefix = get_adk_proxy_url()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!adk web --url_prefix {url_prefix}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rm -rf AUTOSCHOLAR_automation_agent\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rm -rf home_automation_agent","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}